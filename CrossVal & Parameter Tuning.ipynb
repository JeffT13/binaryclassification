{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and HyperParameter Tuning \n",
    "\n",
    "## Goal: Imrpove AUC on Precision emphasized models\n",
    "\n",
    "\n",
    "- format:\n",
    "    - build parameter grid\n",
    "    - build model\n",
    "    - search\n",
    "    - show metrics of solution\n",
    "    \n",
    "    \n",
    "- Models:\n",
    "    - DT\n",
    "    - RF\n",
    "        - w/ and w/out bagging\n",
    "    - LR\n",
    "    - SVM\n",
    "        - lin and poly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Parameter Breakdown\n",
    "    \n",
    "#### Decision Tree\n",
    "    - max_depth\n",
    "    - min_samples_split\n",
    "    - min_samples_leaf\n",
    "    - max_features\n",
    "\n",
    "#### LogReg\n",
    "    - penalty (l1, l2)\n",
    "    - C\n",
    "    - solver (liblinear, saga)\n",
    "    \n",
    "    \n",
    "#### Random Forest (less runs)\n",
    "    - n_estimators\n",
    "    - min_samples_split\n",
    "    - min_samples_leaf\n",
    "    - max_depth\n",
    "    - bootstrap (bool)\n",
    "    \n",
    "\n",
    "#### SVM (not tuned)\n",
    "    - n_neighbors\n",
    "    - weights (uniform (d), distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "\n",
    "#SKLearn\n",
    "import sklearn.model_selection as sk\n",
    "import sklearn.metrics as m\n",
    "from sklearn import ensemble as e\n",
    "from sklearn import svm, linear_model, cluster\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Else\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "#full dfs\n",
    "%store -r df \n",
    "%store -r scaled_df\n",
    "%store -r fe_df\n",
    "%store -r df_upsampled\n",
    "\n",
    "%store -r x_tr\n",
    "%store -r y_tr\n",
    "%store -r x_te\n",
    "%store -r y_te\n",
    "\n",
    "%store -r xs_tr\n",
    "%store -r ys_tr\n",
    "%store -r xs_te\n",
    "%store -r ys_te\n",
    "\n",
    "%store -r x_fetr\n",
    "%store -r y_fetr\n",
    "%store -r x_fete\n",
    "%store -r y_fete\n",
    "%store -r xs_fetr\n",
    "%store -r ys_fetr\n",
    "%store -r xs_fete\n",
    "%store -r ys_fete\n",
    "\n",
    "%store -r x_uptr\n",
    "%store -r y_uptr\n",
    "%store -r x_upte\n",
    "%store -r y_upte\n",
    "%store -r xs_uptr\n",
    "%store -r ys_uptr\n",
    "%store -r xs_upte\n",
    "%store -r ys_upte\n",
    "\n",
    "%store -r cols\n",
    "%store -r fe_cols\n",
    "%store -r testsizepercent\n",
    "%store -r rs\n",
    "\n",
    "\n",
    "#Tuning Parameters\n",
    "crossval = 3 #Stratified by default when target is binary\n",
    "permsz = 100 #count of permutations to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## ---------- Decision Tree\n",
    "\n",
    "# Build Grid\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 9)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split =  [25, 40, 60, 90, 120, 200]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [25, 40, 60, 90, 120, 175]\n",
    "\n",
    "\n",
    "\n",
    "dt_grid = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "dt_s = DecisionTreeClassifier(criterion='entropy').fit(x_tr, y_tr)\n",
    "dt_fe = DecisionTreeClassifier(criterion='entropy').fit(x_fetr, y_fetr)\n",
    "dt_up = DecisionTreeClassifier(criterion='entropy').fit(x_uptr, y_uptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 720 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 171 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 577 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1143 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1873 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2145 out of 2160 | elapsed:   35.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed:   35.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 720 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed:   47.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 720 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:   41.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:   56.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed:  1.0min finished\n"
     ]
    }
   ],
   "source": [
    "# Random Grid Search\n",
    "dt_s_cv = sk.GridSearchCV(estimator = dt_s, \n",
    "                                param_grid= dt_grid, \n",
    "                                cv = crossval, \n",
    "                                verbose=2, n_jobs=-1).fit(x_tr, y_tr)\n",
    "\n",
    "dt_fe_cv = sk.GridSearchCV(estimator = dt_fe, \n",
    "                                param_grid= dt_grid, \n",
    "                                cv = crossval, \n",
    "                                verbose=2, n_jobs=-1).fit(x_fetr, y_fetr)\n",
    "\n",
    "dt_up_cv = sk.GridSearchCV(estimator = dt_up, \n",
    "                                param_grid = dt_grid, \n",
    "                                cv = crossval, \n",
    "                                verbose=2, n_jobs=-1).fit(x_uptr, y_uptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT - s: {'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 25, 'min_samples_split': 60}\n",
      "DT - fe: {'max_depth': 22, 'max_features': 'auto', 'min_samples_leaf': 25, 'min_samples_split': 40}\n",
      "DT - up: {'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 25, 'min_samples_split': 25}\n"
     ]
    }
   ],
   "source": [
    "print(\"DT - s:\", dt_s_cv.best_params_)\n",
    "print(\"DT - fe:\", dt_fe_cv.best_params_)\n",
    "print(\"DT - up:\", dt_up_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## ---------- Random Forest\n",
    "\n",
    "# Build Grid\n",
    "\n",
    "# Number of trees in random forest\n",
    "c = [.0001, .001, .005, .01, .05, .1, 1, 10, 20]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "pen = ['l1', 'l2']\n",
    "\n",
    "solve=['liblinear', 'saga']\n",
    "\n",
    "\n",
    "lr_grid = {'C': c,\n",
    "               'penalty': pen,\n",
    "               'solver': solve}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeffs Laptop\\.conda\\envs\\pfds\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Jeffs Laptop\\.conda\\envs\\pfds\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Jeffs Laptop\\.conda\\envs\\pfds\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_s = linear_model.LogisticRegression(max_iter=500).fit(xs_tr, ys_tr)\n",
    "\n",
    "lr_fe = linear_model.LogisticRegression(max_iter=500).fit(xs_fetr, ys_fetr)\n",
    "\n",
    "lr_up = linear_model.LogisticRegression(max_iter=500).fit(xs_uptr, ys_uptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:   24.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Random Grid Search\n",
    "lr_s_cv = sk.GridSearchCV(estimator = lr_s, \n",
    "                                param_grid = lr_grid, \n",
    "                                cv = crossval, \n",
    "                                verbose=2, n_jobs=-1).fit(xs_tr, ys_tr)\n",
    "\n",
    "lr_fe_cv = sk.GridSearchCV(estimator = lr_fe, \n",
    "                                param_grid = lr_grid, \n",
    "                                cv = crossval, \n",
    "                                verbose=2, n_jobs=-1).fit(xs_fetr, ys_fetr)\n",
    "\n",
    "lr_up_cv = sk.GridSearchCV(estimator = lr_up, \n",
    "                                param_grid = lr_grid, \n",
    "                                cv = crossval, \n",
    "                                verbose=2, n_jobs=-1).fit(xs_uptr, ys_uptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR - s: {'C': 0.05, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "LR - fe: {'C': 20, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "LR - up: {'C': 0.005, 'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "print(\"LR - s:\", lr_s_cv.best_params_)\n",
    "print(\"LR - fe:\", lr_fe_cv.best_params_)\n",
    "print(\"LR - up:\", lr_up_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Tuning\n",
    "\n",
    "-- super slow, leaving out pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## ---------- Random Forest\n",
    "\n",
    "# Build Grid\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 25, stop = 300, num = 8)]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "rf_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "rf_s = e.RandomForestClassifier(criterion='entropy', \n",
    "                                n_estimators=100).fit(x_tr, y_tr)\n",
    "\n",
    "rf_fe = e.RandomForestClassifier(criterion='entropy', \n",
    "                                n_estimators=100).fit(x_fetr, y_fetr)\n",
    "\n",
    "rf_up = e.RandomForestClassifier(criterion='entropy', \n",
    "                                n_estimators=100).fit(x_uptr, y_uptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF - s: {'n_estimators': 64, 'min_samples_split': 25, 'min_samples_leaf': 25, 'max_features': 'auto', 'max_depth': 110, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Random Grid Search\n",
    "rf_s_cv = sk.RandomizedSearchCV(estimator = rf_s, \n",
    "                                param_distributions = rf_grid, \n",
    "                                n_iter = permsz, \n",
    "                                cv = crossval, \n",
    "                                verbose=2, n_jobs=-1).fit(x_tr, y_tr)\n",
    "\n",
    "print(\"RF - s:\", rf_s_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 11.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF - fe: {'n_estimators': 221, 'min_samples_split': 60, 'min_samples_leaf': 25, 'max_features': 'sqrt', 'max_depth': 85, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "rf_fe_cv = sk.RandomizedSearchCV(estimator = rf_fe, \n",
    "                                param_distributions = rf_grid, \n",
    "                                n_iter = permsz, \n",
    "                                cv = crossval, \n",
    "                                verbose=2, n_jobs=-1).fit(x_fetr, y_fetr)\n",
    "\n",
    "print(\"RF - fe:\", rf_fe_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 10.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF - up: {'n_estimators': 221, 'min_samples_split': 25, 'min_samples_leaf': 25, 'max_features': 'auto', 'max_depth': 72, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "rf_up_cv = sk.RandomizedSearchCV(estimator = rf_up, \n",
    "                                param_distributions = rf_grid, \n",
    "                                n_iter = permsz, \n",
    "                                cv = crossval, \n",
    "                                verbose=2, n_jobs=-1).fit(x_uptr, y_uptr)\n",
    "\n",
    "print(\"RF - up:\", rf_up_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
